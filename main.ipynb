{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KiE9W0KnaIa"
      },
      "outputs": [],
      "source": [
        "#code with cloud automation\n",
        "\n",
        "\n",
        "# Install required libraries\n",
        "!pip install google-cloud-storage pandas-gbq opendatasets pandas\n",
        "\n",
        "# Libraries\n",
        "import pandas as pd\n",
        "import opendatasets as od\n",
        "import pandas_gbq\n",
        "import os\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "def initialize_gcs_client():\n",
        "    \"\"\"\n",
        "    Initializes the Google Cloud Storage client.\n",
        "\n",
        "    Returns:\n",
        "        storage.Client: The GCS client object.\n",
        "    \"\"\"\n",
        "    return storage.Client()\n",
        "\n",
        "# Upload a file to GCS\n",
        "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"\n",
        "    Uploads a file to Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "        source_file_name (str): Path to the local file to upload.\n",
        "        destination_blob_name (str): Name of the file in GCS.\n",
        "    \"\"\"\n",
        "    client = initialize_gcs_client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# Download a file from GCS\n",
        "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"\n",
        "    Downloads a file from Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "        source_blob_name (str): Name of the file in GCS.\n",
        "        destination_file_name (str): Path to save the downloaded file.\n",
        "    \"\"\"\n",
        "    client = initialize_gcs_client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
        "\n",
        "# Save cleaned data to GCS\n",
        "def save_cleaned_data_to_gcs(data, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Saves cleaned data to Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The cleaned dataset.\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "        file_name (str): Name of the file in GCS.\n",
        "    \"\"\"\n",
        "    local_file = \"/tmp/cleaned_data.csv\"  # Temporary local file\n",
        "    data.to_csv(local_file, index=False)\n",
        "    upload_to_gcs(bucket_name, local_file, file_name)\n",
        "\n",
        "# Load raw data from GCS\n",
        "def load_raw_data_from_gcs(bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Loads raw data from Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "        file_name (str): Name of the file in GCS.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The raw dataset.\n",
        "    \"\"\"\n",
        "    local_file = \"/tmp/raw_data.csv\"  # Temporary local file\n",
        "    download_from_gcs(bucket_name, file_name, local_file)\n",
        "    return pd.read_csv(local_file)\n",
        "\n",
        "# Extract data\n",
        "def extract_data():\n",
        "    \"\"\"\n",
        "    Downloads the dataset from Kaggle and loads it into a pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The extracted dataset.\n",
        "    \"\"\"\n",
        "    od.download(\"https://www.kaggle.com/datasets/carrie1/ecommerce-data/data\")\n",
        "    try:\n",
        "        data = pd.read_csv(\"/content/ecommerce-data/data.csv\", encoding='latin-1')\n",
        "    except UnicodeDecodeError:\n",
        "        data = pd.read_csv(\"/content/ecommerce-data/data.csv\", encoding='ISO-8859-1')\n",
        "    return data\n",
        "\n",
        "# Inspect data\n",
        "def inspect_data(data):\n",
        "    \"\"\"\n",
        "    Inspects the dataset for basic information and missing values.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to inspect.\n",
        "    \"\"\"\n",
        "    print(\"Dataset Head:\")\n",
        "    print(data.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(data.info())\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "# Clean data\n",
        "def clean_data(data):\n",
        "    \"\"\"\n",
        "    Cleans the dataset by handling missing values, removing invalid rows, and dropping duplicates.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to clean.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned dataset.\n",
        "    \"\"\"\n",
        "    data.dropna(subset=['CustomerID', 'Description'], inplace=True)\n",
        "    data['Description'].fillna('Unknown', inplace=True)\n",
        "    data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "    data = data[data['Quantity'] > 0]\n",
        "    data = data[data['UnitPrice'] > 0]\n",
        "    data.drop_duplicates(inplace=True)\n",
        "    return data\n",
        "\n",
        "# Transform data\n",
        "def transform_data(data):\n",
        "    \"\"\"\n",
        "    Transforms the dataset by calculating total revenue, sales by country, and sales by category.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to transform.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The transformed dataset.\n",
        "    \"\"\"\n",
        "    data['TotalRevenue'] = data['Quantity'] * data['UnitPrice']\n",
        "    total_revenue = data['TotalRevenue'].sum()\n",
        "    print(f\"Total Revenue: ${total_revenue: .2f}\")\n",
        "\n",
        "    sales_per_country = data.groupby('Country')['InvoiceNo'].nunique().reset_index(name='NumberOfSales')\n",
        "    print(\"\\nSales by Country:\")\n",
        "    print(sales_per_country)\n",
        "\n",
        "    sales_by_category = data.groupby('Description')['TotalRevenue'].sum().reset_index(name='TotalRevenue')\n",
        "    print(\"\\nSales by Category:\")\n",
        "    print(sales_by_category.sort_values(by='TotalRevenue', ascending=False))\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load data into BigQuery\n",
        "def load_data(data, project_id, dataset_id, table_id, credentials_path):\n",
        "    \"\"\"\n",
        "    Loads the transformed data into Google BigQuery.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to load.\n",
        "        project_id (str): The GCP project ID.\n",
        "        dataset_id (str): The BigQuery dataset ID.\n",
        "        table_id (str): The BigQuery table ID.\n",
        "        credentials_path (str): Path to the GCP service account key.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pandas_gbq.to_gbq(\n",
        "            data,\n",
        "            destination_table=f\"{dataset_id}.{table_id}\",\n",
        "            project_id=project_id,\n",
        "            if_exists=\"replace\",\n",
        "            credentials_path=credentials_path\n",
        "        )\n",
        "        print(f\"Data uploaded to BigQuery: {dataset_id}.{table_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data into BigQuery: {e}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Set up\n",
        "    project_id = os.getenv(\"GCP_PROJECT_ID\")\n",
        "    dataset_id = os.getenv(\"GCP_DATASET_ID\")\n",
        "    table_id = os.getenv(\"GCP_TABLE_ID\")\n",
        "    credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    bucket_name = \"amentoria\"  # Replace with your bucket name\n",
        "\n",
        "    # Step 1: Extract Data\n",
        "    print(\"Extracting data...\")\n",
        "    data = extract_data()\n",
        "\n",
        "    # Step 2: Inspect Data\n",
        "    print(\"\\nInspecting data...\")\n",
        "    inspect_data(data)\n",
        "\n",
        "    # Step 3: Clean Data\n",
        "    print(\"\\nCleaning data...\")\n",
        "    data = clean_data(data)\n",
        "\n",
        "    # Step 4: Save Cleaned Data to GCS\n",
        "    cleaned_data_file = \"cleaned_data.csv\"\n",
        "    save_cleaned_data_to_gcs(data, bucket_name, cleaned_data_file)\n",
        "\n",
        "    # Step 5: Transform Data\n",
        "    print(\"\\nTransforming data...\")\n",
        "    data = transform_data(data)\n",
        "\n",
        "    # Step 6: Load Data into BigQuery\n",
        "    print(\"\\nLoading data into BigQuery...\")\n",
        "    load_data(data, project_id, dataset_id, table_id, credentials_path)\n",
        "\n",
        "    print(\"\\nETL pipeline completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
